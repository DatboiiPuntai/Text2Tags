{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q bitsandbytes datasets accelerate loralib editdistance sentencepiece\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, LlamaTokenizer, LlamaForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from utils import similar_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "peft_model_id = r\"ooferdoodles/text2tags-opt-1.3b\"\n",
    "base_model = \"facebook/opt-1.3b\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    return_dict=True,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, torch_dtype=torch.float16)\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files=r\"dataset/test_data.json\")\n",
    "tag_dict = similar_tag.load_dict()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    typical_p=1,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=300,\n",
    "    use_cache=True,\n",
    "    no_repeat_ngram_size=3,\n",
    "    # pad_token_id=model.config.eos_token_id\n",
    "    # truncation_length=2048,\n",
    "    # min_length=0,\n",
    "    # add_bos_token=True,\n",
    "    # ban_eos_token=False,\n",
    "    # skip_special_tokens=True,\n",
    "    # stopping_strings=[],\n",
    "    # penalty_alpha=0,\n",
    "    # repetition_penalty=2.5,\n",
    "    # encoder_repetition_penalty=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe(data_point):\n",
    "    # max_new_tokens = int(len(data_point['caption_string']) * max_token_scale)\n",
    "    prompt = f\"### Caption: {data_point['caption_string']}\\n### Tags: \"\n",
    "    tokenized_prompt = tokenizer.encode(\n",
    "        prompt, return_tensors='pt', add_special_tokens=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(\n",
    "            input_ids=tokenized_prompt['input_ids'],\n",
    "            generation_config=generation_config,\n",
    "        )[0]\n",
    "    preds = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "    pred_list = [x.strip() for x in preds.split('### Tags:')[-1].split(\",\")]\n",
    "    corrected_tags = similar_tag.correct_tags(pred_list, tag_dict)\n",
    "    data_point['tags'] = data_point['tag_string'].split(', ')\n",
    "    data_point['pred_tags'] = corrected_tags\n",
    "    return data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = data.map(pipe)\n",
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_point):\n",
    "    correct_count = len(set(data_point['tags']).intersection(data_point['pred_tags']))\n",
    "    # incorrect_count = len(data_point['tags']) - correct_count\n",
    "    data_point['accuracy'] = correct_count / len(data_point['tags']) * 100\n",
    "    return data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_data = processed_data.map(evaluate_accuracy)\n",
    "evaluated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluated_data['train'].to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['accuracy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred_tags'][145]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
