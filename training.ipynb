{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q bitsandbytes datasets accelerate loralib sentencepiece scikit-learn\n",
    "!pip install tensorboardX\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Users\\panta\\anaconda3\\envs\\nlp\\bin\\cudart64_110.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:152: UserWarning: C:\\Users\\panta\\anaconda3\\envs\\nlp did not contain ['cudart64_110.dll', 'cudart64_120.dll'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:152: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('C:/Users/panta/anaconda3/envs/nlp/Library/usr/bin'), WindowsPath('C:/Users/panta/anaconda3/envs/nlp/Library/mingw-w64/bin')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"facebook/opt-350m\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#     BASE_MODEL,\n",
    "#     load_in_8bit=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1572864 || all params: 332769280 || trainable%: 0.472659014678278\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_int8_training(model)\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample(item, max_seq_length=1024, add_eos_token=True):\n",
    "        result = tokenizer(\n",
    "            item,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=True,\n",
    "        )\n",
    "        result = {\n",
    "            \"input_ids\": result[\"input_ids\"][:-1],\n",
    "            \"attention_mask\": result[\"attention_mask\"][:-1],\n",
    "        }\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < max_seq_length\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Describe the caption with appropriate tags\n",
    "### Input:\n",
    "{data_point['caption_string']}\n",
    "### Response:\n",
    "{data_point['tag_string']}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/panta/.cache/huggingface/datasets/json/default-fac367448397b4f6/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16553c047c1461399cb4f63f33fb700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\panta\\.cache\\huggingface\\datasets\\json\\default-fac367448397b4f6\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-a753b907c50531b5.arrow and C:\\Users\\panta\\.cache\\huggingface\\datasets\\json\\default-fac367448397b4f6\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-402f08e87db1eb6b.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fea51d2e23f4d51adc4d51f2bacda4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18952 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3f362b4d95451ca138fee134384696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['caption_string', 'tag_string', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 18952\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['caption_string', 'tag_string', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 998\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files=r'dataset/train_data.json')\n",
    "data = data[\"train\"].train_test_split(test_size=0.05, shuffle=True, seed=42)\n",
    "data = data.map(lambda x: tokenize_sample(generate_prompt(x)))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiLabelBinarizer(classes=[&#x27;1girl&#x27;, &#x27;solo&#x27;, &#x27;long_hair&#x27;, &#x27;breasts&#x27;, &#x27;blush&#x27;,\n",
       "                             &#x27;looking_at_viewer&#x27;, &#x27;smile&#x27;, &#x27;short_hair&#x27;,\n",
       "                             &#x27;open_mouth&#x27;, &#x27;bangs&#x27;, &#x27;blue_eyes&#x27;,\n",
       "                             &#x27;multiple_girls&#x27;, &#x27;blonde_hair&#x27;, &#x27;skirt&#x27;,\n",
       "                             &#x27;brown_hair&#x27;, &#x27;large_breasts&#x27;, &#x27;simple_background&#x27;,\n",
       "                             &#x27;black_hair&#x27;, &#x27;eyebrows_visible_through_hair&#x27;,\n",
       "                             &#x27;thighhighs&#x27;, &#x27;hair_ornament&#x27;, &#x27;hat&#x27;, &#x27;red_eyes&#x27;,\n",
       "                             &#x27;gloves&#x27;, &#x27;shirt&#x27;, &#x27;touhou&#x27;, &#x27;1boy&#x27;, &#x27;dress&#x27;,\n",
       "                             &#x27;white_background&#x27;, &#x27;original&#x27;, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiLabelBinarizer</label><div class=\"sk-toggleable__content\"><pre>MultiLabelBinarizer(classes=[&#x27;1girl&#x27;, &#x27;solo&#x27;, &#x27;long_hair&#x27;, &#x27;breasts&#x27;, &#x27;blush&#x27;,\n",
       "                             &#x27;looking_at_viewer&#x27;, &#x27;smile&#x27;, &#x27;short_hair&#x27;,\n",
       "                             &#x27;open_mouth&#x27;, &#x27;bangs&#x27;, &#x27;blue_eyes&#x27;,\n",
       "                             &#x27;multiple_girls&#x27;, &#x27;blonde_hair&#x27;, &#x27;skirt&#x27;,\n",
       "                             &#x27;brown_hair&#x27;, &#x27;large_breasts&#x27;, &#x27;simple_background&#x27;,\n",
       "                             &#x27;black_hair&#x27;, &#x27;eyebrows_visible_through_hair&#x27;,\n",
       "                             &#x27;thighhighs&#x27;, &#x27;hair_ornament&#x27;, &#x27;hat&#x27;, &#x27;red_eyes&#x27;,\n",
       "                             &#x27;gloves&#x27;, &#x27;shirt&#x27;, &#x27;touhou&#x27;, &#x27;1boy&#x27;, &#x27;dress&#x27;,\n",
       "                             &#x27;white_background&#x27;, &#x27;original&#x27;, ...])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiLabelBinarizer(classes=['1girl', 'solo', 'long_hair', 'breasts', 'blush',\n",
       "                             'looking_at_viewer', 'smile', 'short_hair',\n",
       "                             'open_mouth', 'bangs', 'blue_eyes',\n",
       "                             'multiple_girls', 'blonde_hair', 'skirt',\n",
       "                             'brown_hair', 'large_breasts', 'simple_background',\n",
       "                             'black_hair', 'eyebrows_visible_through_hair',\n",
       "                             'thighhighs', 'hair_ornament', 'hat', 'red_eyes',\n",
       "                             'gloves', 'shirt', 'touhou', '1boy', 'dress',\n",
       "                             'white_background', 'original', ...])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "tag_list = open(r'dictionaries/tag_dict.txt').read().splitlines()\n",
    "mlb = MultiLabelBinarizer(classes=tag_list)\n",
    "mlb.fit([list(tag_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from utils import similar_tag\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)[0]\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)[0]\n",
    "\n",
    "    pred_tags = [x.strip() for x in decoded_preds.split(\",\")]\n",
    "    pred_tags_corrected = similar_tag.correct_tags(pred_tags, tag_list)\n",
    "\n",
    "    tags = [x.strip() for x in decoded_labels.split(\",\")]\n",
    "\n",
    "    one_hots_pred = mlb.transform([pred_tags_corrected])\n",
    "    one_hots_truth = mlb.transform([tags])\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    accuracy = accuracy_score(y_true=one_hots_truth, y_pred=one_hots_pred)\n",
    "    recall = recall_score(\n",
    "        y_true=one_hots_truth, y_pred=one_hots_pred, average=\"weighted\", zero_division=1\n",
    "    )\n",
    "    precision = precision_score(\n",
    "        y_true=one_hots_truth, y_pred=one_hots_pred, average=\"weighted\", zero_division=1\n",
    "    )\n",
    "    f1_micro = f1_score(\n",
    "        y_true=one_hots_truth, y_pred=one_hots_pred, average=\"micro\", zero_division=1\n",
    "    )\n",
    "    f1_macro = f1_score(\n",
    "        y_true=one_hots_truth, y_pred=one_hots_pred, average=\"macro\", zero_division=1\n",
    "    )\n",
    "    f1_weighted = f1_score(\n",
    "        y_true=one_hots_truth, y_pred=one_hots_pred, average=\"weighted\", zero_division=1\n",
    "    )\n",
    "\n",
    "    results[\"accuracy\"] = accuracy\n",
    "    results[\"recall\"] = recall\n",
    "    results[\"precision\"] = precision\n",
    "    results[\"f1_micro\"] = f1_micro\n",
    "    results[\"f1_macro\"] = f1_macro\n",
    "    results[\"f1_weighted\"] = f1_weighted\n",
    "\n",
    "    return {k: round(v, 4) for k, v in results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/opt-350m\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=32,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    logging_steps=5,\n",
    "    report_to=\"tensorboard\",\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset=data['test'],\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    args=training_args\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17159099d7334e68895515a10c2fa0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4971, 'learning_rate': 9.887387387387388e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2936, 'learning_rate': 9.774774774774775e-05, 'epoch': 0.07}\n",
      "{'loss': 3.163, 'learning_rate': 9.662162162162163e-05, 'epoch': 0.1}\n",
      "{'loss': 3.0409, 'learning_rate': 9.54954954954955e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50acc9e775e44e595024417fa51416d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8741016387939453, 'eval_runtime': 70.6738, 'eval_samples_per_second': 14.121, 'eval_steps_per_second': 1.769, 'epoch': 0.14}\n",
      "{'loss': 2.9388, 'learning_rate': 9.436936936936938e-05, 'epoch': 0.17}\n",
      "{'loss': 2.8321, 'learning_rate': 9.324324324324324e-05, 'epoch': 0.2}\n",
      "{'loss': 2.7495, 'learning_rate': 9.211711711711712e-05, 'epoch': 0.24}\n",
      "{'loss': 2.6662, 'learning_rate': 9.0990990990991e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187d4eaae4fb40ca873342a05e97fe1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.491180658340454, 'eval_runtime': 90.4867, 'eval_samples_per_second': 11.029, 'eval_steps_per_second': 1.381, 'epoch': 0.27}\n",
      "{'loss': 2.5768, 'learning_rate': 8.986486486486487e-05, 'epoch': 0.3}\n",
      "{'loss': 2.5281, 'learning_rate': 8.873873873873875e-05, 'epoch': 0.34}\n",
      "{'loss': 2.4579, 'learning_rate': 8.761261261261262e-05, 'epoch': 0.37}\n",
      "{'loss': 2.4141, 'learning_rate': 8.64864864864865e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6109c469fed40afa75763558505847f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3061065673828125, 'eval_runtime': 87.0345, 'eval_samples_per_second': 11.467, 'eval_steps_per_second': 1.436, 'epoch': 0.41}\n",
      "{'loss': 2.3803, 'learning_rate': 8.536036036036036e-05, 'epoch': 0.44}\n",
      "{'loss': 2.3599, 'learning_rate': 8.423423423423423e-05, 'epoch': 0.47}\n",
      "{'loss': 2.3505, 'learning_rate': 8.310810810810811e-05, 'epoch': 0.51}\n",
      "{'loss': 2.2951, 'learning_rate': 8.198198198198198e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71521d793f741c4b3936957433dfd72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2188708782196045, 'eval_runtime': 82.8665, 'eval_samples_per_second': 12.043, 'eval_steps_per_second': 1.508, 'epoch': 0.54}\n",
      "{'loss': 2.298, 'learning_rate': 8.085585585585586e-05, 'epoch': 0.57}\n",
      "{'loss': 2.2781, 'learning_rate': 7.972972972972974e-05, 'epoch': 0.61}\n",
      "{'loss': 2.2612, 'learning_rate': 7.86036036036036e-05, 'epoch': 0.64}\n",
      "{'loss': 2.2449, 'learning_rate': 7.747747747747748e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c278bee1c943fd8f066a53a88cf253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1609365940093994, 'eval_runtime': 80.4803, 'eval_samples_per_second': 12.401, 'eval_steps_per_second': 1.553, 'epoch': 0.68}\n",
      "{'loss': 2.2251, 'learning_rate': 7.635135135135135e-05, 'epoch': 0.71}\n",
      "{'loss': 2.2174, 'learning_rate': 7.522522522522523e-05, 'epoch': 0.74}\n",
      "{'loss': 2.1983, 'learning_rate': 7.40990990990991e-05, 'epoch': 0.78}\n",
      "{'loss': 2.1872, 'learning_rate': 7.297297297297297e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e219ef5df74c078104932675a3ac5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1142821311950684, 'eval_runtime': 85.5385, 'eval_samples_per_second': 11.667, 'eval_steps_per_second': 1.461, 'epoch': 0.81}\n",
      "{'loss': 2.1741, 'learning_rate': 7.184684684684685e-05, 'epoch': 0.84}\n",
      "{'loss': 2.1958, 'learning_rate': 7.072072072072072e-05, 'epoch': 0.88}\n",
      "{'loss': 2.1606, 'learning_rate': 6.95945945945946e-05, 'epoch': 0.91}\n",
      "{'loss': 2.1568, 'learning_rate': 6.846846846846847e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd933ffd7f7b4984a00ce544c42690eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0761427879333496, 'eval_runtime': 85.5941, 'eval_samples_per_second': 11.66, 'eval_steps_per_second': 1.46, 'epoch': 0.95}\n",
      "{'loss': 2.1448, 'learning_rate': 6.734234234234235e-05, 'epoch': 0.98}\n",
      "{'loss': 2.1517, 'learning_rate': 6.621621621621621e-05, 'epoch': 1.01}\n",
      "{'loss': 2.1354, 'learning_rate': 6.50900900900901e-05, 'epoch': 1.05}\n",
      "{'loss': 2.1109, 'learning_rate': 6.396396396396397e-05, 'epoch': 1.08}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aecf5dec42ff436d8c80884ad621be27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0454249382019043, 'eval_runtime': 89.3975, 'eval_samples_per_second': 11.164, 'eval_steps_per_second': 1.398, 'epoch': 1.08}\n",
      "{'loss': 2.1246, 'learning_rate': 6.283783783783784e-05, 'epoch': 1.11}\n",
      "{'loss': 2.118, 'learning_rate': 6.171171171171172e-05, 'epoch': 1.15}\n",
      "{'loss': 2.1187, 'learning_rate': 6.058558558558559e-05, 'epoch': 1.18}\n",
      "{'loss': 2.0823, 'learning_rate': 5.9459459459459466e-05, 'epoch': 1.22}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3473fdafc83b40f9ac1b9d2593a1fde2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0186610221862793, 'eval_runtime': 84.45, 'eval_samples_per_second': 11.818, 'eval_steps_per_second': 1.48, 'epoch': 1.22}\n",
      "{'loss': 2.0991, 'learning_rate': 5.833333333333334e-05, 'epoch': 1.25}\n",
      "{'loss': 2.0839, 'learning_rate': 5.720720720720721e-05, 'epoch': 1.28}\n",
      "{'loss': 2.0743, 'learning_rate': 5.6081081081081086e-05, 'epoch': 1.32}\n",
      "{'loss': 2.0739, 'learning_rate': 5.4954954954954966e-05, 'epoch': 1.35}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad4390bc1c442b4b3b2cf03fab07beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9957807064056396, 'eval_runtime': 79.681, 'eval_samples_per_second': 12.525, 'eval_steps_per_second': 1.569, 'epoch': 1.35}\n",
      "{'loss': 2.0784, 'learning_rate': 5.382882882882884e-05, 'epoch': 1.38}\n",
      "{'loss': 2.0837, 'learning_rate': 5.27027027027027e-05, 'epoch': 1.42}\n",
      "{'loss': 2.0726, 'learning_rate': 5.157657657657657e-05, 'epoch': 1.45}\n",
      "{'loss': 2.0255, 'learning_rate': 5.0450450450450445e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67666ad6420642fb9ab37c5211bd3bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.976782202720642, 'eval_runtime': 91.888, 'eval_samples_per_second': 10.861, 'eval_steps_per_second': 1.36, 'epoch': 1.49}\n",
      "{'loss': 2.062, 'learning_rate': 4.9324324324324325e-05, 'epoch': 1.52}\n",
      "{'loss': 2.0324, 'learning_rate': 4.8198198198198205e-05, 'epoch': 1.55}\n",
      "{'loss': 2.0526, 'learning_rate': 4.707207207207208e-05, 'epoch': 1.59}\n",
      "{'loss': 2.0259, 'learning_rate': 4.594594594594595e-05, 'epoch': 1.62}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90117855756481caa1b482148b67b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9602158069610596, 'eval_runtime': 79.9097, 'eval_samples_per_second': 12.489, 'eval_steps_per_second': 1.564, 'epoch': 1.62}\n",
      "{'loss': 2.0345, 'learning_rate': 4.481981981981982e-05, 'epoch': 1.65}\n",
      "{'loss': 2.0225, 'learning_rate': 4.369369369369369e-05, 'epoch': 1.69}\n",
      "{'loss': 2.0366, 'learning_rate': 4.256756756756757e-05, 'epoch': 1.72}\n",
      "{'loss': 2.0276, 'learning_rate': 4.1441441441441444e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ab49a668154d68980f1b8beda0e04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9456040859222412, 'eval_runtime': 80.9117, 'eval_samples_per_second': 12.334, 'eval_steps_per_second': 1.545, 'epoch': 1.76}\n",
      "{'loss': 2.0225, 'learning_rate': 4.031531531531532e-05, 'epoch': 1.79}\n",
      "{'loss': 2.0252, 'learning_rate': 3.918918918918919e-05, 'epoch': 1.82}\n",
      "{'loss': 2.0021, 'learning_rate': 3.8063063063063064e-05, 'epoch': 1.86}\n",
      "{'loss': 2.0136, 'learning_rate': 3.693693693693694e-05, 'epoch': 1.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee766438bb84cef8cfca7107fae38a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9331198930740356, 'eval_runtime': 84.1341, 'eval_samples_per_second': 11.862, 'eval_steps_per_second': 1.486, 'epoch': 1.89}\n",
      "{'loss': 2.0026, 'learning_rate': 3.581081081081081e-05, 'epoch': 1.92}\n",
      "{'loss': 2.0153, 'learning_rate': 3.468468468468469e-05, 'epoch': 1.96}\n",
      "{'loss': 1.9974, 'learning_rate': 3.355855855855856e-05, 'epoch': 1.99}\n",
      "{'loss': 1.9959, 'learning_rate': 3.2432432432432436e-05, 'epoch': 2.03}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43be19ae9924c2ca9392968ba6a4cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.923833966255188, 'eval_runtime': 81.2243, 'eval_samples_per_second': 12.287, 'eval_steps_per_second': 1.539, 'epoch': 2.03}\n",
      "{'loss': 2.0192, 'learning_rate': 3.130630630630631e-05, 'epoch': 2.06}\n",
      "{'loss': 1.9965, 'learning_rate': 3.0180180180180183e-05, 'epoch': 2.09}\n",
      "{'loss': 1.9958, 'learning_rate': 2.9054054054054052e-05, 'epoch': 2.13}\n",
      "{'loss': 1.9957, 'learning_rate': 2.7927927927927926e-05, 'epoch': 2.16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d23574d581410a9762f2228e6532d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.914947748184204, 'eval_runtime': 80.6746, 'eval_samples_per_second': 12.371, 'eval_steps_per_second': 1.549, 'epoch': 2.16}\n",
      "{'loss': 1.9829, 'learning_rate': 2.6801801801801802e-05, 'epoch': 2.2}\n",
      "{'loss': 1.9927, 'learning_rate': 2.5675675675675675e-05, 'epoch': 2.23}\n",
      "{'loss': 1.9936, 'learning_rate': 2.454954954954955e-05, 'epoch': 2.26}\n",
      "{'loss': 1.9944, 'learning_rate': 2.3423423423423425e-05, 'epoch': 2.3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c2a7173fe843aa84888a5e62452368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9083406925201416, 'eval_runtime': 81.3256, 'eval_samples_per_second': 12.272, 'eval_steps_per_second': 1.537, 'epoch': 2.3}\n",
      "{'loss': 1.9697, 'learning_rate': 2.2297297297297298e-05, 'epoch': 2.33}\n",
      "{'loss': 1.9896, 'learning_rate': 2.117117117117117e-05, 'epoch': 2.36}\n",
      "{'loss': 1.9764, 'learning_rate': 2.0045045045045048e-05, 'epoch': 2.4}\n",
      "{'loss': 1.9707, 'learning_rate': 1.891891891891892e-05, 'epoch': 2.43}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577cdd276328472ba28fd18074633063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9014346599578857, 'eval_runtime': 75.6538, 'eval_samples_per_second': 13.192, 'eval_steps_per_second': 1.652, 'epoch': 2.43}\n",
      "{'loss': 1.977, 'learning_rate': 1.779279279279279e-05, 'epoch': 2.47}\n",
      "{'loss': 1.9749, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.5}\n",
      "{'loss': 1.9784, 'learning_rate': 1.554054054054054e-05, 'epoch': 2.53}\n",
      "{'loss': 1.9792, 'learning_rate': 1.4414414414414416e-05, 'epoch': 2.57}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4756e0562a4e4dc38727c871449fabf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8964929580688477, 'eval_runtime': 72.1081, 'eval_samples_per_second': 13.84, 'eval_steps_per_second': 1.734, 'epoch': 2.57}\n",
      "{'loss': 1.9662, 'learning_rate': 1.3288288288288289e-05, 'epoch': 2.6}\n",
      "{'loss': 1.9962, 'learning_rate': 1.2162162162162164e-05, 'epoch': 2.63}\n",
      "{'loss': 1.9788, 'learning_rate': 1.1036036036036037e-05, 'epoch': 2.67}\n",
      "{'loss': 1.966, 'learning_rate': 9.90990990990991e-06, 'epoch': 2.7}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fdf6235a514728973e65aec4ccfdce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8935216665267944, 'eval_runtime': 142.6651, 'eval_samples_per_second': 6.995, 'eval_steps_per_second': 0.876, 'epoch': 2.7}\n",
      "{'loss': 1.9605, 'learning_rate': 8.783783783783785e-06, 'epoch': 2.74}\n",
      "{'loss': 1.9534, 'learning_rate': 7.657657657657658e-06, 'epoch': 2.77}\n",
      "{'loss': 1.9773, 'learning_rate': 6.531531531531532e-06, 'epoch': 2.8}\n",
      "{'loss': 1.9688, 'learning_rate': 5.405405405405406e-06, 'epoch': 2.84}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37c910722bc4fa6bdcb0669d8aae74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8915883302688599, 'eval_runtime': 106.0494, 'eval_samples_per_second': 9.411, 'eval_steps_per_second': 1.179, 'epoch': 2.84}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1667\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:1930\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1928\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1929\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1930\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1932\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1933\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1934\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1935\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1936\u001b[0m ):\n\u001b[0;32m   1937\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1938\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:2710\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2707\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m   2709\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_grad_scaling:\n\u001b[1;32m-> 2710\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m   2711\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_apex:\n\u001b[0;32m   2712\u001b[0m     \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mscale_loss(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer) \u001b[39mas\u001b[39;00m scaled_loss:\n",
      "File \u001b[1;32mc:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Describe the caption with appropriate tags\n",
    "### Input:\n",
    "Minato Aqua, the virtual youtuber from hololive is wearing a blue maid outfit with maid cap and her pink and blue streaked hair is styled in twintails\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Describe the caption with appropriate tags\n",
      "### Input:\n",
      "Minato Aqua, the virtual youtuber from hololive is wearing a blue maid outfit with maid cap and her pink and blue streaked hair is styled in twintails\n",
      "### Response:\n",
      "1girl, blue_hair, blue-eyed_girl, maid_cap, maid-cap, pink_hair_and_blush, pink, blue, pink-eyes, pink_(curly_hair), maid_caps, maid_(curlish), maid-panties, maid(curly), maid(pant)\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(text, return_tensors='pt').to(\"cuda\")\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "  output_tokens = model.generate(**batch, max_new_tokens=200, no_repeat_ngram_size=3)\n",
    "\n",
    "print(tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"ooferdoodles/text2tags-opt-350m\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(r\"loras/opt-350m-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
