{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q bitsandbytes datasets accelerate loralib sentencepiece scikit-learn\n",
    "!pip install tensorboardX\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_MODEL = \"facebook/opt-350m\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     BASE_MODEL,\n",
    "#     load_in_8bit=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_int8_training(model)\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample(item, max_seq_length=1024, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        item,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding=True,\n",
    "    )\n",
    "    result = {\n",
    "        \"input_ids\": result[\"input_ids\"][:-1],\n",
    "        \"attention_mask\": result[\"attention_mask\"][:-1],\n",
    "    }\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < max_seq_length\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"### Caption: {data_point['caption_string']}\\n### Tags: {data_point['tag_string']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files=r'dataset/train_data.json')\n",
    "data = data[\"train\"].train_test_split(test_size=0.05, shuffle=True, seed=42)\n",
    "data = data.map(lambda x: tokenize_sample(generate_prompt(x)))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "tag_list = open(r'dictionaries/tag_dict.txt').read().splitlines()\n",
    "mlb = MultiLabelBinarizer(classes=tag_list)\n",
    "mlb.fit([list(tag_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from utils import similar_tag\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)[0]\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)[0]\n",
    "\n",
    "    pred_tags = [x.strip() for x in decoded_preds.split(\",\")]\n",
    "    pred_tags_corrected = similar_tag.correct_tags(pred_tags, tag_list)\n",
    "\n",
    "    tags = [x.strip() for x in decoded_labels.split(\",\")]\n",
    "\n",
    "    one_hots_pred = mlb.transform([pred_tags_corrected])\n",
    "    one_hots_truth = mlb.transform([tags])\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    accuracy = accuracy_score(y_true=one_hots_truth, y_pred=one_hots_pred)\n",
    "    recall = recall_score(\n",
    "        y_true=one_hots_truth, y_pred=one_hots_pred, average=\"weighted\", zero_division=1\n",
    "    )\n",
    "    precision = precision_score(\n",
    "        y_true=one_hots_truth, y_pred=one_hots_pred, average=\"weighted\", zero_division=1\n",
    "    )\n",
    "    f1_micro = f1_score(\n",
    "        y_true=one_hots_truth, y_pred=one_hots_pred, average=\"micro\", zero_division=1\n",
    "    )\n",
    "    f1_macro = f1_score(\n",
    "        y_true=one_hots_truth, y_pred=one_hots_pred, average=\"macro\", zero_division=1\n",
    "    )\n",
    "    f1_weighted = f1_score(\n",
    "        y_true=one_hots_truth, y_pred=one_hots_pred, average=\"weighted\", zero_division=1\n",
    "    )\n",
    "\n",
    "    results[\"accuracy\"] = accuracy\n",
    "    results[\"recall\"] = recall\n",
    "    results[\"precision\"] = precision\n",
    "    results[\"f1_micro\"] = f1_micro\n",
    "    results[\"f1_macro\"] = f1_macro\n",
    "    results[\"f1_weighted\"] = f1_weighted\n",
    "\n",
    "    return {k: round(v, 4) for k, v in results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/opt-350m\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=32,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    logging_steps=5,\n",
    "    report_to=\"tensorboard\",\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset=data['test'],\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    args=training_args\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Describe the caption with appropriate tags\n",
    "### Input:\n",
    "Minato Aqua, the virtual youtuber from hololive is wearing a blue maid outfit with maid cap and her pink and blue streaked hair is styled in twintails\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(text, return_tensors='pt').to(\"cuda\")\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "  output_tokens = model.generate(**batch, max_new_tokens=200, no_repeat_ngram_size=3)\n",
    "\n",
    "print(tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
