{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Users\\panta\\anaconda3\\envs\\nlp\\bin\\cudart64_110.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:152: UserWarning: C:\\Users\\panta\\anaconda3\\envs\\nlp did not contain ['cudart64_110.dll', 'cudart64_120.dll'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:152: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('C:/Users/panta/anaconda3/envs/nlp/Library/usr/bin'), WindowsPath('C:/Users/panta/anaconda3/envs/nlp/Library/mingw-w64/bin')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer  # noqa: F402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = 'decapoda-research/llama-7b-hf'\n",
    "LORA_MODEL = 'ooferdoodles/text2tags-llama7b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c6874e75ca414eaa5e13a0acfd722c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": \"cpu\"},\n",
    ")\n",
    "\n",
    "first_weight = base_model.model.layers[0].self_attn.q_proj.weight\n",
    "first_weight_old = first_weight.clone()\n",
    "\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    LORA_MODEL,\n",
    "    device_map={\"\": \"cpu\"},\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "lora_weight = lora_model.base_model.model.model.layers[\n",
    "    0\n",
    "].self_attn.q_proj.weight\n",
    "\n",
    "assert torch.allclose(first_weight_old, first_weight)\n",
    "\n",
    "# merge weights - new merging method from peft\n",
    "lora_model = lora_model.merge_and_unload()\n",
    "\n",
    "lora_model.train(False)\n",
    "\n",
    "# did we do anything?\n",
    "assert not torch.allclose(first_weight_old, first_weight)\n",
    "\n",
    "lora_model_sd = lora_model.state_dict()\n",
    "deloreanized_sd = {\n",
    "    k.replace(\"base_model.model.\", \"\"): v\n",
    "    for k, v in lora_model_sd.items()\n",
    "    if \"lora\" not in k\n",
    "}\n",
    "\n",
    "LlamaForCausalLM.save_pretrained(\n",
    "    base_model, \"./alpaca-tagger\", state_dict=deloreanized_sd, max_shard_size=\"400MB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
