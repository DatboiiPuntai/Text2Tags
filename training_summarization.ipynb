{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install -q bitsandbytes datasets accelerate loralib sentencepiece\n",
    "!pip install tensorboardX\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Users\\panta\\anaconda3\\envs\\nlp\\bin\\cudart64_110.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:152: UserWarning: C:\\Users\\panta\\anaconda3\\envs\\nlp did not contain ['cudart64_110.dll', 'cudart64_120.dll'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:152: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('C:/Users/panta/anaconda3/envs/nlp/Library/usr/bin'), WindowsPath('C:/Users/panta/anaconda3/envs/nlp/Library/mingw-w64/bin')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"google/flan-t5-small\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    # load_in_8bit=True,\n",
    "    # torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 688128 || all params: 77649280 || trainable%: 0.8862001038515747\n"
     ]
    }
   ],
   "source": [
    "# model = prepare_model_for_int8_training(model)\n",
    "# config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"q\", \"v\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"SEQ_2_SEQ_LM\",\n",
    "# )\n",
    "# model = get_peft_model(model, config)\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/panta/.cache/huggingface/datasets/json/default-fac367448397b4f6/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f8a1095a514189be657f5a96a51ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['caption_string', 'tag_string'],\n",
       "        num_rows: 19950\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files=r'dataset/train_data.json')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"caption_string\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"tag_string\"], max_length=1024, truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\panta\\.cache\\huggingface\\datasets\\json\\default-fac367448397b4f6\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-a753b907c50531b5.arrow and C:\\Users\\panta\\.cache\\huggingface\\datasets\\json\\default-fac367448397b4f6\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-402f08e87db1eb6b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\panta\\.cache\\huggingface\\datasets\\json\\default-fac367448397b4f6\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-f45342928e41ed12.arrow\n",
      "Loading cached processed dataset at C:\\Users\\panta\\.cache\\huggingface\\datasets\\json\\default-fac367448397b4f6\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-47cef8e61ebab212.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['caption_string', 'tag_string', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 18952\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['caption_string', 'tag_string', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 998\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data = data[\"train\"].train_test_split(test_size=0.05, shuffle=True, seed=42)\n",
    "tokenized_data = split_data.map(preprocess_function, batched=True)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "label_pad_token_id = -100\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"outputs/flan-t5\",\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    metric_for_best_model=\"f1_weighted\",\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"tensorboard\",\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiLabelBinarizer(classes=[&#x27;1girl&#x27;, &#x27;solo&#x27;, &#x27;long_hair&#x27;, &#x27;breasts&#x27;, &#x27;blush&#x27;,\n",
       "                             &#x27;looking_at_viewer&#x27;, &#x27;smile&#x27;, &#x27;short_hair&#x27;,\n",
       "                             &#x27;open_mouth&#x27;, &#x27;bangs&#x27;, &#x27;blue_eyes&#x27;,\n",
       "                             &#x27;multiple_girls&#x27;, &#x27;blonde_hair&#x27;, &#x27;skirt&#x27;,\n",
       "                             &#x27;brown_hair&#x27;, &#x27;large_breasts&#x27;, &#x27;simple_background&#x27;,\n",
       "                             &#x27;black_hair&#x27;, &#x27;eyebrows_visible_through_hair&#x27;,\n",
       "                             &#x27;thighhighs&#x27;, &#x27;hair_ornament&#x27;, &#x27;hat&#x27;, &#x27;red_eyes&#x27;,\n",
       "                             &#x27;gloves&#x27;, &#x27;shirt&#x27;, &#x27;touhou&#x27;, &#x27;1boy&#x27;, &#x27;dress&#x27;,\n",
       "                             &#x27;white_background&#x27;, &#x27;original&#x27;, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiLabelBinarizer</label><div class=\"sk-toggleable__content\"><pre>MultiLabelBinarizer(classes=[&#x27;1girl&#x27;, &#x27;solo&#x27;, &#x27;long_hair&#x27;, &#x27;breasts&#x27;, &#x27;blush&#x27;,\n",
       "                             &#x27;looking_at_viewer&#x27;, &#x27;smile&#x27;, &#x27;short_hair&#x27;,\n",
       "                             &#x27;open_mouth&#x27;, &#x27;bangs&#x27;, &#x27;blue_eyes&#x27;,\n",
       "                             &#x27;multiple_girls&#x27;, &#x27;blonde_hair&#x27;, &#x27;skirt&#x27;,\n",
       "                             &#x27;brown_hair&#x27;, &#x27;large_breasts&#x27;, &#x27;simple_background&#x27;,\n",
       "                             &#x27;black_hair&#x27;, &#x27;eyebrows_visible_through_hair&#x27;,\n",
       "                             &#x27;thighhighs&#x27;, &#x27;hair_ornament&#x27;, &#x27;hat&#x27;, &#x27;red_eyes&#x27;,\n",
       "                             &#x27;gloves&#x27;, &#x27;shirt&#x27;, &#x27;touhou&#x27;, &#x27;1boy&#x27;, &#x27;dress&#x27;,\n",
       "                             &#x27;white_background&#x27;, &#x27;original&#x27;, ...])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiLabelBinarizer(classes=['1girl', 'solo', 'long_hair', 'breasts', 'blush',\n",
       "                             'looking_at_viewer', 'smile', 'short_hair',\n",
       "                             'open_mouth', 'bangs', 'blue_eyes',\n",
       "                             'multiple_girls', 'blonde_hair', 'skirt',\n",
       "                             'brown_hair', 'large_breasts', 'simple_background',\n",
       "                             'black_hair', 'eyebrows_visible_through_hair',\n",
       "                             'thighhighs', 'hair_ornament', 'hat', 'red_eyes',\n",
       "                             'gloves', 'shirt', 'touhou', '1boy', 'dress',\n",
       "                             'white_background', 'original', ...])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "tag_list = open(r'dictionaries/tag_dict.txt').read().splitlines()\n",
    "mlb = MultiLabelBinarizer(classes=tag_list)\n",
    "mlb.fit([list(tag_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from utils import similar_tag\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)[0]\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)[0]\n",
    "\n",
    "    pred_tags = [x.strip() for x in decoded_preds.split(\",\")]\n",
    "    pred_tags_corrected = similar_tag.correct_tags(pred_tags, tag_list)\n",
    "\n",
    "    tags = [x.strip() for x in decoded_labels.split(\",\")]\n",
    "\n",
    "    one_hots_pred = mlb.transform([pred_tags_corrected])\n",
    "    one_hots_truth = mlb.transform([tags])\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    accuracy = accuracy_score(y_true=one_hots_truth, y_pred=one_hots_pred)\n",
    "    recall = recall_score(\n",
    "        y_true=one_hots_truth, y_pred=one_hots_pred, average=\"weighted\", zero_division=1\n",
    "    )\n",
    "    precision = precision_score(\n",
    "        y_true=one_hots_truth, y_pred=one_hots_pred, average=\"weighted\", zero_division=1\n",
    "    )\n",
    "    f1_micro = f1_score(\n",
    "        y_true=one_hots_truth, y_pred=one_hots_pred, average=\"micro\", zero_division=1\n",
    "    )\n",
    "    f1_macro = f1_score(\n",
    "        y_true=one_hots_truth, y_pred=one_hots_pred, average=\"macro\", zero_division=1\n",
    "    )\n",
    "    f1_weighted = f1_score(\n",
    "        y_true=one_hots_truth, y_pred=one_hots_pred, average=\"weighted\", zero_division=1\n",
    "    )\n",
    "\n",
    "    results[\"accuracy\"] = accuracy\n",
    "    results[\"recall\"] = recall\n",
    "    results[\"precision\"] = precision\n",
    "    results[\"f1_micro\"] = f1_micro\n",
    "    results[\"f1_macro\"] = f1_macro\n",
    "    results[\"f1_weighted\"] = f1_weighted\n",
    "\n",
    "    return {k: round(v, 4) for k, v in results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[transformers.EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89238f45886b452e902005b94683de6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11845 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4054, 'learning_rate': 0.0002873364288729421, 'epoch': 0.21}\n",
      "{'loss': 1.968, 'learning_rate': 0.0002746728577458843, 'epoch': 0.42}\n",
      "{'loss': 1.8148, 'learning_rate': 0.0002620092866188265, 'epoch': 0.63}\n",
      "{'loss': 1.7189, 'learning_rate': 0.00024934571549176863, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\utils.py:1250: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0ca9d57e854863a7a2047dada933a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11389)\n",
      "{'eval_loss': 1.340453863143921, 'eval_accuracy': 0.0, 'eval_recall': 0.0882, 'eval_precision': 1.0, 'eval_f1_micro': 0.1579, 'eval_f1_macro': 0.9972, 'eval_f1_weighted': 0.0882, 'eval_runtime': 218.6781, 'eval_samples_per_second': 4.564, 'eval_steps_per_second': 0.572, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6581, 'learning_rate': 0.00023668214436471083, 'epoch': 1.06}\n",
      "{'loss': 1.5984, 'learning_rate': 0.000224018573237653, 'epoch': 1.27}\n",
      "{'loss': 1.5491, 'learning_rate': 0.00021135500211059518, 'epoch': 1.48}\n",
      "{'loss': 1.5106, 'learning_rate': 0.00019869143098353735, 'epoch': 1.69}\n",
      "{'loss': 1.4912, 'learning_rate': 0.00018602785985647952, 'epoch': 1.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ae315ba14d4c0cbc478240ddf047c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11389)\n",
      "{'eval_loss': 1.1723272800445557, 'eval_accuracy': 0.0, 'eval_recall': 0.0588, 'eval_precision': 1.0, 'eval_f1_micro': 0.1026, 'eval_f1_macro': 0.9969, 'eval_f1_weighted': 0.0588, 'eval_runtime': 210.6145, 'eval_samples_per_second': 4.739, 'eval_steps_per_second': 0.594, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4683, 'learning_rate': 0.00017336428872942168, 'epoch': 2.11}\n",
      "{'loss': 1.4288, 'learning_rate': 0.00016070071760236385, 'epoch': 2.32}\n",
      "{'loss': 1.4132, 'learning_rate': 0.000148037146475306, 'epoch': 2.53}\n",
      "{'loss': 1.4065, 'learning_rate': 0.0001353735753482482, 'epoch': 2.74}\n",
      "{'loss': 1.3848, 'learning_rate': 0.00012271000422119037, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea5c545612848a1854d7ca3b93c9077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11389)\n",
      "{'eval_loss': 1.0993340015411377, 'eval_accuracy': 0.0, 'eval_recall': 0.0882, 'eval_precision': 1.0, 'eval_f1_micro': 0.1622, 'eval_f1_macro': 0.9973, 'eval_f1_weighted': 0.0882, 'eval_runtime': 214.9226, 'eval_samples_per_second': 4.644, 'eval_steps_per_second': 0.582, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3734, 'learning_rate': 0.00011004643309413255, 'epoch': 3.17}\n",
      "{'loss': 1.3753, 'learning_rate': 9.73828619670747e-05, 'epoch': 3.38}\n",
      "{'loss': 1.3524, 'learning_rate': 8.471929084001688e-05, 'epoch': 3.59}\n",
      "{'loss': 1.3319, 'learning_rate': 7.205571971295904e-05, 'epoch': 3.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97d7454bfd345cd8f7b6b2030582124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11389)\n",
      "{'eval_loss': 1.0612291097640991, 'eval_accuracy': 0.0, 'eval_recall': 0.0882, 'eval_precision': 1.0, 'eval_f1_micro': 0.15, 'eval_f1_macro': 0.997, 'eval_f1_weighted': 0.0882, 'eval_runtime': 212.3929, 'eval_samples_per_second': 4.699, 'eval_steps_per_second': 0.589, 'epoch': 4.0}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Loading a quantized checkpoint into non-quantized Linear8bitLt is not supported. Please call module.cuda() before module.load_state_dict()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1667\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\accelerate\\utils\\memory.py:124\u001b[0m, in \u001b[0;36mfind_executable_batch_size.<locals>.decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo executable batch size found, reached zero.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    123\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 124\u001b[0m     \u001b[39mreturn\u001b[39;00m function(batch_size, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    125\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    126\u001b[0m     \u001b[39mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "File \u001b[1;32mc:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:2050\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2047\u001b[0m     \u001b[39melif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2048\u001b[0m         smp\u001b[39m.\u001b[39mbarrier()\n\u001b[1;32m-> 2050\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_best_model()\n\u001b[0;32m   2052\u001b[0m \u001b[39m# add remaining tr_loss\u001b[39;00m\n\u001b[0;32m   2053\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_total_loss_scalar \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:2226\u001b[0m, in \u001b[0;36mTrainer._load_best_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2221\u001b[0m         state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(best_model_path, map_location\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2223\u001b[0m     \u001b[39m# If the model is on the GPU, it still works!\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m     \u001b[39m# workaround for FSDP bug https://github.com/pytorch/pytorch/issues/82963\u001b[39;00m\n\u001b[0;32m   2225\u001b[0m     \u001b[39m# which takes *args instead of **kwargs\u001b[39;00m\n\u001b[1;32m-> 2226\u001b[0m     load_result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mload_state_dict(state_dict, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m   2227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2228\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_issue_warnings_after_load(load_result)\n",
      "File \u001b[1;32mc:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:2027\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2020\u001b[0m         out \u001b[39m=\u001b[39m hook(module, incompatible_keys)\n\u001b[0;32m   2021\u001b[0m         \u001b[39massert\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m, (\n\u001b[0;32m   2022\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2023\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2024\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mit should be done inplace.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2025\u001b[0m         )\n\u001b[1;32m-> 2027\u001b[0m load(\u001b[39mself\u001b[39;49m, state_dict)\n\u001b[0;32m   2028\u001b[0m \u001b[39mdel\u001b[39;00m load\n\u001b[0;32m   2030\u001b[0m \u001b[39mif\u001b[39;00m strict:\n",
      "File \u001b[1;32mc:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:2015\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[1;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[0;32m   2013\u001b[0m         child_prefix \u001b[39m=\u001b[39m prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2014\u001b[0m         child_state_dict \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m local_state_dict\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k\u001b[39m.\u001b[39mstartswith(child_prefix)}\n\u001b[1;32m-> 2015\u001b[0m         load(child, child_state_dict, child_prefix)\n\u001b[0;32m   2017\u001b[0m \u001b[39m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[0;32m   2018\u001b[0m incompatible_keys \u001b[39m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[1;32mc:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:2015\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[1;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[0;32m   2013\u001b[0m         child_prefix \u001b[39m=\u001b[39m prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2014\u001b[0m         child_state_dict \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m local_state_dict\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k\u001b[39m.\u001b[39mstartswith(child_prefix)}\n\u001b[1;32m-> 2015\u001b[0m         load(child, child_state_dict, child_prefix)\n\u001b[0;32m   2017\u001b[0m \u001b[39m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[0;32m   2018\u001b[0m incompatible_keys \u001b[39m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "    \u001b[1;31m[... skipping similar frames: Module.load_state_dict.<locals>.load at line 2015 (6 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:2015\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[1;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[0;32m   2013\u001b[0m         child_prefix \u001b[39m=\u001b[39m prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2014\u001b[0m         child_state_dict \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m local_state_dict\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k\u001b[39m.\u001b[39mstartswith(child_prefix)}\n\u001b[1;32m-> 2015\u001b[0m         load(child, child_state_dict, child_prefix)\n\u001b[0;32m   2017\u001b[0m \u001b[39m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[0;32m   2018\u001b[0m incompatible_keys \u001b[39m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[1;32mc:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:2009\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[1;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(module, local_state_dict, prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m   2008\u001b[0m     local_metadata \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m metadata \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m metadata\u001b[39m.\u001b[39mget(prefix[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], {})\n\u001b[1;32m-> 2009\u001b[0m     module\u001b[39m.\u001b[39;49m_load_from_state_dict(\n\u001b[0;32m   2010\u001b[0m         local_state_dict, prefix, local_metadata, \u001b[39mTrue\u001b[39;49;00m, missing_keys, unexpected_keys, error_msgs)\n\u001b[0;32m   2011\u001b[0m     \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   2012\u001b[0m         \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\panta\\anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:298\u001b[0m, in \u001b[0;36mLinear8bitLt._load_from_state_dict\u001b[1;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[39mif\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    296\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mSCB \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m         \u001b[39m# buffers not yet initialized, can't call them directly without\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLoading a quantized checkpoint into non-quantized Linear8bitLt is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mnot supported. Please call module.cuda() before module.load_state_dict()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    301\u001b[0m     input_param \u001b[39m=\u001b[39m state_dict[key]\n\u001b[0;32m    302\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mSCB\u001b[39m.\u001b[39mcopy_(input_param)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Loading a quantized checkpoint into non-quantized Linear8bitLt is not supported. Please call module.cuda() before module.load_state_dict()"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"summarize: Minato Aqua and Hoshimachi Suisei, virtual youtubers from hololive are wearing a deep blue maid outfit with maid cap with pink and blue streaked hair styled in twintails\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForSeq2SeqLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Your max_length is set to 60, but you input_length is only 55. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': '1girl, blue_hair, deep_hair, maid, maid_cap, maid_cap, maid_cap, maid_cap, maid_cap, maid_cap, maid_cap, maid_cap, hololive,'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = transformers.pipeline(\"summarization\", model=model, tokenizer=tokenizer,max_length=60)\n",
    "summarizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
